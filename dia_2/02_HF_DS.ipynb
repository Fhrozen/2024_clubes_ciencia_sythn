{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace y Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones de Instalacion\n",
    "\n",
    "Antes de empezar, verificar si tienen instalado las librerias necesarias.\n",
    "En su configuracion personal, pueden instalar desde el directorio base del repositorio usando `pip install -r installer/requirements.txt`,\n",
    "ya sea para Windows o Linux.\n",
    "\n",
    "En caso de usar Colab, pueden copiar el contenido en un archivo dentro de Colab, o en su propia nube.\n",
    "Ingresar la linea de codigo: `!pip install -r <folder>/requirements.txt` dentro de una celda, y presionar en `Ejecutar celda`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace_Hub\n",
    "\n",
    "Hugging Face Hub, el sitio web principal, es una plataforma central que permite a cualquiera descubrir, utilizar y contribuir con nuevos modelos y conjuntos de datos de última generación. Alberga una amplia variedad de modelos, con más de 10.000 disponibles públicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "CACHE_DIR=\"../models\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"facebook/musicgen-small\",\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/musicgen-small\",\n",
    "    cache_dir=CACHE_DIR,\n",
    ").to(device)\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "\n",
    "caption = \"\"\n",
    "extras = {}\n",
    "\n",
    "inputs = processor(\n",
    "    text=[caption],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    **extras\n",
    ").to(device)\n",
    "\n",
    "audio_out = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=1024).squeeze()\n",
    "audio_out = audio_out.cpu().numpy()\n",
    "name_out = \"generated/music_out.wav\"\n",
    "sf.write(name_out, audio_out, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con otro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def spectrogram_from_image(\n",
    "    image: Image.Image,\n",
    "    power: float = 0.25,\n",
    "    stereo: bool = False,\n",
    "    max_value: float = 30e6,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a spectrogram magnitude array from a spectrogram image.\n",
    "\n",
    "    This is the inverse of image_from_spectrogram, except for discretization error from\n",
    "    quantizing to uint8.\n",
    "\n",
    "    Args:\n",
    "        image: (frequency, time, channels)\n",
    "        power: The power curve applied to the spectrogram\n",
    "        stereo: Whether the spectrogram encodes stereo data\n",
    "        max_value: The max value of the original spectrogram. In practice doesn't matter.\n",
    "\n",
    "    Returns:\n",
    "        spectrogram: (channels, frequency, time)\n",
    "    \"\"\"\n",
    "    # Convert to RGB if single channel\n",
    "    if image.mode in (\"P\", \"L\"):\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Flip Y\n",
    "    image = image.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
    "\n",
    "    # Munge channels into a numpy array of (channels, frequency, time)\n",
    "    data = np.array(image).transpose(2, 0, 1)\n",
    "    if stereo:\n",
    "        # Take the G and B channels as done in image_from_spectrogram\n",
    "        data = data[[1, 2], :, :]\n",
    "    else:\n",
    "        data = data[0:1, :, :]\n",
    "\n",
    "    # Convert to floats\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    # Invert\n",
    "    data = 255 - data\n",
    "\n",
    "    # Rescale to 0-1\n",
    "    data = data / 255\n",
    "\n",
    "    # Reverse the power curve\n",
    "    data = np.power(data, 1 / power)\n",
    "\n",
    "    # Rescale to max value\n",
    "    data = data * max_value\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_inverter(n_fft, num_griffin_lim_iters, win_length, hop_length, device):\n",
    "    inverse_spectrogram_func = torchaudio.transforms.GriffinLim(\n",
    "        n_fft=n_fft,\n",
    "        n_iter=num_griffin_lim_iters,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        window_fn=torch.hann_window,\n",
    "        power=1.0,\n",
    "        wkwargs=None,\n",
    "        momentum=0.99,\n",
    "        length=None,\n",
    "        rand_init=True,\n",
    "    ).to(device)\n",
    "    return inverse_spectrogram_func\n",
    "\n",
    "\n",
    "def audio_from_spectrogram(\n",
    "    self,\n",
    "    spectrogram: np.ndarray,\n",
    "    apply_filters: bool = True,\n",
    "    normalize: bool = True, \n",
    "):\n",
    "    \"\"\"\n",
    "    Reconstruct an audio segment from a spectrogram.\n",
    "\n",
    "    Args:\n",
    "        spectrogram: (batch, frequency, time)\n",
    "        apply_filters: Post-process with normalization and compression\n",
    "\n",
    "    Returns:\n",
    "        audio: Audio segment with channels equal to the batch dimension\n",
    "    \"\"\"\n",
    "    # Move to device\n",
    "    amplitudes_mel = torch.from_numpy(spectrogram).to(self.device)\n",
    "\n",
    "    # Reconstruct the waveform\n",
    "    waveform = waveform_from_mel_amplitudes(amplitudes_mel)\n",
    "\n",
    "    # Convert to audio segment\n",
    "    if normalize:\n",
    "        waveform *= np.iinfo(np.int16).max / np.max(np.abs(waveform))\n",
    "        \n",
    "    # apply filers:\n",
    "    # compression (effects normalize)\n",
    "    # compress dynamic range\n",
    "    # librosa.mu_compress\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# model =  model.to_bettertransformer()\n",
    "pipe.enable_cpu_offload()\n",
    "\n",
    "mensaje = \"\"\n",
    "# sampling_rate = model.generation_config.sample_rate\n",
    "\n",
    "image = pipe(mensaje).images[0]\n",
    "PIL_image = Image.fromarray(np.uint8(image)).convert('RGB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos de imagen a Sonido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value: float = 30e6\n",
    "power_for_image: float = 0.25\n",
    "stereo: bool = False\n",
    "\n",
    "spectrogram = spectrogram_from_image(\n",
    "    image,\n",
    "    max_value=max_value,\n",
    "    power=power_for_image,\n",
    "    stereo=stereo,\n",
    ")\n",
    "\n",
    "segment = audio_from_spectrogram(\n",
    "    spectrogram,\n",
    "    apply_filters=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora cargamos bark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from datetime import datetime\n",
    "from transformers import AutoProcessor, BarkModel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"suno/bark-small\",\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "model = BarkModel.from_pretrained(\n",
    "    \"suno/bark\",\n",
    "    cache_dir=\"./models\",\n",
    ").to(device)\n",
    "\n",
    "model.enable_cpu_offload()\n",
    "sampling_rate = model.generation_config.sample_rate\n",
    "\n",
    "captions = \"\"\n",
    "voice_preset = \"v2/es_speaker_1\"\n",
    "\n",
    "inputs = processor(caption, voice_preset=voice_preset).to(device)\n",
    "audio_out = model.generate(**inputs).squeeze().cpu().numpy()\n",
    "\n",
    "time_now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "name_out = f\"generated/speech_{time_now}.wav\"\n",
    "sf.write(name_out, audio_out, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente Descargamos T5-small y revisamos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-small\")\n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets es una biblioteca para acceder y compartir fácilmente conjuntos de datos para tareas de audio, visión por computadora y procesamiento del lenguaje natural (NLP).\n",
    "\n",
    "Cargue un conjunto (base) de datos en una sola línea de código y utilice los potentes métodos de procesamiento de datos para preparar rápidamente su conjunto de datos para entrenarlo en un modelo de aprendizaje profundo. Con el respaldo del formato Apache Arrow, procese grandes conjuntos de datos con lecturas sin copia y sin restricciones de memoria para lograr una velocidad y eficiencia óptimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ylacombe/google-chilean-spanish\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando un conjunto de datos de texto, preprocesamos utilizando diferente funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "\n",
    "dataset[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

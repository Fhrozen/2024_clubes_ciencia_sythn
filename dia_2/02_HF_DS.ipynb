{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace y Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones de Instalacion\n",
    "\n",
    "Antes de empezar, verificar si tienen instalado las librerias necesarias.\n",
    "En su configuracion personal, pueden instalar desde el directorio base del repositorio usando `pip install -r installer/requirements.txt`,\n",
    "ya sea para Windows o Linux.\n",
    "\n",
    "En caso de usar Colab, pueden copiar el contenido en un archivo dentro de Colab, o en su propia nube.\n",
    "Ingresar la linea de codigo: `!pip install -r <folder>/requirements.txt` dentro de una celda, y presionar en `Ejecutar celda`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace_Hub\n",
    "\n",
    "Hugging Face Hub, el sitio web principal, es una plataforma central que permite a cualquiera descubrir, utilizar y contribuir con nuevos modelos y conjuntos de datos de última generación. Alberga una amplia variedad de modelos, con más de 10.000 disponibles públicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "CACHE_DIR=\"../models\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"facebook/musicgen-small\",\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/musicgen-small\",\n",
    "    cache_dir=CACHE_DIR,\n",
    ").to(device)\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "\n",
    "caption = \"\"\n",
    "extras = {}\n",
    "\n",
    "inputs = processor(\n",
    "    text=[caption],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    **extras\n",
    ").to(device)\n",
    "\n",
    "audio_out = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=1024).squeeze()\n",
    "audio_out = audio_out.cpu().numpy()\n",
    "name_out = \"generated/music_out.wav\"\n",
    "sf.write(name_out, audio_out, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con otro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "import extras_libs\n",
    "from matplotlib import pyplot as plt\n",
    "from argparse import Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"riffusion/riffusion-model-v1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "pipe.unet = extras_libs.get_unet_traced(CACHE_DIR, device, dtype)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# num_inference_steps = 50\n",
    "alpha = 0.5\n",
    "start = Namespace(\n",
    "    # prompt=\"Okay, Up with it girl, rock with it girl, Swang with me\",\n",
    "    prompt=\"piano melodic pop\",\n",
    "    seed=42,\n",
    "    denoising=0.75,\n",
    "    guidance=7.0\n",
    ")\n",
    "end = Namespace(\n",
    "    prompt=\"modified to be sang by vocals\",\n",
    "    seed=123,\n",
    "    denoising=0.75,\n",
    "    guidance=7.0\n",
    ")\n",
    "\n",
    "image = extras_libs.forward_riffuse_pipeline(pipe, device, dtype, start, end, alpha=alpha)\n",
    "plt.imshow(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos de imagen a Sonido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = extras_libs.image2audio(image, device)\n",
    "Audio(audio, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora cargamos bark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from datetime import datetime\n",
    "from transformers import AutoProcessor, BarkModel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"suno/bark-small\",\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "model = BarkModel.from_pretrained(\n",
    "    \"suno/bark\",\n",
    "    cache_dir=\"./models\",\n",
    ").to(device)\n",
    "\n",
    "model.enable_cpu_offload()\n",
    "sampling_rate = model.generation_config.sample_rate\n",
    "\n",
    "captions = \"\"\n",
    "voice_preset = \"v2/es_speaker_1\"\n",
    "\n",
    "inputs = processor(caption, voice_preset=voice_preset).to(device)\n",
    "audio_out = model.generate(**inputs).squeeze().cpu().numpy()\n",
    "\n",
    "time_now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "name_out = f\"generated/speech_{time_now}.wav\"\n",
    "sf.write(name_out, audio_out, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente Descargamos T5-small y revisamos su salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-small\")\n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets es una biblioteca para acceder y compartir fácilmente conjuntos de datos para tareas de audio, visión por computadora y procesamiento del lenguaje natural (NLP).\n",
    "\n",
    "Cargue un conjunto (base) de datos en una sola línea de código y utilice los potentes métodos de procesamiento de datos para preparar rápidamente su conjunto de datos para entrenarlo en un modelo de aprendizaje profundo. Con el respaldo del formato Apache Arrow, procese grandes conjuntos de datos con lecturas sin copia y sin restricciones de memoria para lograr una velocidad y eficiencia óptimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ylacombe/google-chilean-spanish\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando un conjunto de datos de texto, preprocesamos utilizando diferente funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "\n",
    "dataset[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinamiento de un modelo (T5) usando librerias HuggingFace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccione un modelo preentrenado para refinarlo\n",
    "\n",
    "Elija el modelo T5 para texto en Ingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar la base de datos\n",
    "\n",
    "Para acortar el tiempo de ejecución de este ejemplo, carguemos solo las primeras 5000 instancias de la base de datos seleccionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación del conjunto de datos\n",
    "\n",
    "Para preparar la base de datos para el entrenamiento y evaluación, cree los diccionarios necesarios. Estos serán útiles al realizar inferencias y para obtener información de metadatos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "labels = dataset.features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion, cargue el procesador de texto usado en el preentrenamiendo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"\"] = [train_transforms() for  in example_batch[\"\"]]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"\"] = [val_transforms() for  in example_batch[\"\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide la base de datos para entrenamiento y validacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "splits = dataset.train_test_split(test_size=0.1)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, configure las funciones de transformación para la base de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparacion del modelo\n",
    "\n",
    "Antes de cargar el modelo, definamos una función auxiliar para verificar la cantidad total de parámetros que tiene un modelo, así como cuántos de ellos se pueden entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Params entrenables: {trainable_params} || Totalidad: {all_param} || entrenables%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante inicializar el modelo original correctamente, ya que se utilizará como base para crear el PeftModel que realmente ajustará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import , TrainingArguments, Trainer\n",
    "\n",
    "model = .from_pretrained(\n",
    "    model_checkpoint,\n",
    "\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de crear un PeftModel, puede verificar la cantidad de parámetros entrenables en el modelo original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, utilice `get_peft_model` para ajustar el modelo base de modo que se agreguen matrices de \"actualización\" en los lugares respectivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analicemos lo que está pasando aquí. Para usar LoRA, debe especificar los módulos de destino en `LoraConfig` para que `get_peft_model()` sepa qué módulos dentro de nuestro modelo deben modificarse con matrices LoRA. En este ejemplo, solo nos interesa apuntar a las matrices de consulta y valor de los bloques de atención del modelo base. Dado que los parámetros correspondientes a estas matrices se \"llaman\" \"consulta\" y \"valor\" respectivamente, los especificamos en consecuencia en el argumento target_modules de LoraConfig.\n",
    "\n",
    "También especificamos `module_to_save`. Después de envolver el modelo base con `get_peft_model()` junto con la configuración, obtenemos un nuevo modelo en el que solo se pueden entrenar los parámetros LoRA (las llamadas \"matrices de actualización\") mientras que los parámetros previamente entrenados se mantienen congelados. Sin embargo, queremos que los parámetros del clasificador también se entrenen al ajustar el modelo base en nuestro conjunto de datos personalizado. Para garantizar que los parámetros del clasificador también estén entrenados, especificamos `module_to_save`. Esto también garantiza que estos módulos se serialicen junto con los parámetros entrenables de LoRA cuando se utilizan utilidades como `save_pretrained()` y` push_to_hub()`.\n",
    "\n",
    "Esto es lo que significan los otros parámetros:\n",
    "\n",
    "`r`: La dimensión utilizada por las matrices de actualización de LoRA.\n",
    "`alfa`: factor de escala.\n",
    "`bias`: especifica si los parámetros bias (tendencia) deben entrenarse. Ninguno indica que no se entrenará ninguno de los parámetros bias.\n",
    "`r` y `alpha` juntos controlan la cantidad total de parámetros finales entrenables cuando se usa LoRA, lo que le brinda la flexibilidad de equilibrar el equilibrio entre el rendimiento final y la eficiencia informática.\n",
    "\n",
    "Al observar la cantidad de parámetros entrenables, puede ver cuántos parámetros estamos entrenando realmente. Dado que el objetivo es lograr un ajuste fino eficiente en los parámetros, debería esperar ver menos parámetros entrenables en lora_model en comparación con el modelo original, que de hecho es el caso aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiendo parametros de entrenamiento\n",
    "\n",
    "Para ajustar el modelo, utilice `Trainer`. Acepta varios argumentos que puedes ajustar usando `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 128\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-lora-food101\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparación con los métodos que no son PEFT, puede utilizar un tamaño de lote mayor ya que hay menos parámetros para entrenar. También puedes establecer una tasa de aprendizaje mayor que la normal (`1e-5` por ejemplo).\n",
    "\n",
    "Potencialmente, esto también puede reducir la necesidad de realizar costosos experimentos de ajuste de hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparar métrica de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calcula la precision en el subconjunto de predicciones.\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función Compute_metrics toma una tupla con nombre como entrada: `predictions`, que son los logits del modelo como matrices Numpy, y `label_ids`, que son las etiquetas de verdad fundamental como matrices Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiendo la función de intercalación\n",
    "\n",
    "`Trainer` utiliza una función de intercalación para recopilar un lote de ejemplos de capacitación y evaluación y prepararlos en un formato que sea aceptable para el modelo subyacente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento y evaluacion\n",
    "\n",
    "Reúna todo: modelo, argumentos de entrenamiento, datos, función de intercalación, etc. Luego, ¡comience el entrenamiento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    lora_model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecuta una inferencia\n",
    "\n",
    "Veamos cómo cargar los parámetros actualizados de LoRA junto con nuestro modelo base para inferencia. Cuando envuelve un modelo base con PeftModel, las modificaciones se realizan in situ. Para mitigar cualquier inquietud que pueda surgir de las modificaciones locales, inicialice el modelo base tal como lo hizo antes y construya el modelo de inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(repo_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "# Load the LoRA model\n",
    "inference_model = PeftModel.from_pretrained(model, repo_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Finalmente, ejecute la inferencia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = inference_model(**encoding)\n",
    "    logits = outputs.logits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
